# Natural Language Processing Systems course at NaUKMA

## General Info and Course Description
- Overview of traditional NLP methods, Recurrent Neural Nets, Seq-to-Seq models, attention mechanism and transformers, and Large Language Models and their practical applications with a focus on the latter.
- 8 weeks of work with a lecture and a practice period each week. There may be an invited lecturer from Grammarly to delve deeper into some of the concepts of the topic of your interest.
- Attendance is highly desired but not obligatory. Timely submission of home assignments as possible is also encouraged, although not forced.

## Topics and Timeline
As the course is completely fresh, the topics to be covered, assignments, structure, etc. may be subject to change (in rational measures)
- Topic 1. Word Vectors, Word Embeddings, CBOW model.
- Topic 2. Language Models and Recurrent Neural Networks.
- Topic 3. Attention Mechanism and Transformers.
- Topic 4. Large Language Models and their Applications.

For each topic, there is a home assignment - usually a jupyter notebook with additional util scripts and detailed description of the task provided. The student is aimed to complete the coding tasks and express an opinion on certain problems if prompted to. The maximum number of points per assignment is 15p. 

## Evaluation
- 4 H/A make it a total 60p of the total mark.
- A short oral presentation (up to 15m) of the selected NLP research paper gives a max of 10p.
- The final exam, or rather the alternative group project assignment (similar to a Kaggle competition) yields the remaining 30p.
  
| Activity              | Max. Points  |
| --------------------- | ------------ |
| 4 Home Assignments    | 60           |
| Research Presentation | 10           |
| Final Group Project   | 30           |

## Assignment 1
In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis. The assignemnt can be found [here](https://github.com/dmytro-kuzmenko/nlp-systems-kma/tree/main/Assignment%201)
